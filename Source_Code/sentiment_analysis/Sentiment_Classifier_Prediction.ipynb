{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                               \n",
    "#                Sentiment Prediction                           \n",
    "#                Team: BDII                                   \n",
    "#                Course:EECS6893                      \n",
    "#                Columbia University                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Noteï¼›This part is implemented on databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classifier Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"dbfs:/FileStore/tables/Sentiment_Analysis_Dataset-08265.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sqlContext.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"delimiter\", \",\")\\\n",
    "  .load(path)\n",
    "\n",
    "data.cache()\n",
    "data=data.dropna()\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.count()\n",
    "sqlContext.registerDataFrameAsTable(data, \"table1\")\n",
    "df2 = sqlContext.sql(\"SELECT Sentiment, count(*) from table1 group by Sentiment\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"SentimentText\", outputCol=\"words\")\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(data)\n",
    "tokenized.select(\"SentimentText\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"SentimentText\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(data)\n",
    "regexTokenized.select(\"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_raw\")\n",
    "removed = remover.transform(regexTokenized)\n",
    "removed.select(\"filtered_raw\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remover_adv = StopWordsRemover(inputCol=\"filtered_raw\", outputCol=\"filtered\", stopWords=[\"\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"m\",\"kaggle\",\"etc\",\"though\",\"man\",\"too\",\"so\",\"rain\",\"shower\",\"000\",\"http\",\n",
    "                                                                                        \"day\", \"quot\",\"com\",\"im\",\"it\",\"get\",\"bit\",\"see\"])\n",
    "removed_adv = remover_adv.transform(removed)\n",
    "removed_adv.dropna()\n",
    "removed_adv.select(\"filtered\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = removed_adv.select(\"filtered\")\n",
    "display(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "featurizedData = hashingTF.transform(removed_adv)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "#rescaledData.select(\"title\", \"features\").show()\n",
    "svms = rescaledData.selectExpr(\"Sentiment as label\", \"features\")\n",
    "svms.dropna()\n",
    "svms.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes,NaiveBayesModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.util import MLUtils\n",
    "splits = svms.randomSplit([0.85, 0.15],12)  \n",
    "train = splits[0]  \n",
    "test = splits[1]  \n",
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\") \n",
    "model = nb.fit(train)\n",
    "\n",
    "result = model.transform(test)  \n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")  \n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")  \n",
    "\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictionAndLabels)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularize Class - tweet_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tweet_sentiment():\n",
    "  \n",
    "  def __init__(self, path_x):\n",
    "    self.dataframe=None\n",
    "    self.regexTokenized=None\n",
    "    self.stop_removed=None\n",
    "    self.tweet_removed_adv=None\n",
    "    self.tweet_svms=None\n",
    "    self.model=None\n",
    "    self.positive=0\n",
    "    self.negative=0\n",
    "    self.path=path_x\n",
    "  \n",
    "  def data_loading(self):\n",
    "    tweet_data = sqlContext.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"false\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .load(self.path)\n",
    "    tweet_data.cache()\n",
    "    tweet_data=tweet_data.dropna()\n",
    "    self.dataframe=tweet_data\n",
    "    return tweet_data\n",
    "  \n",
    "  def nb_model(self, nb_model):\n",
    "    self.model=nb_model\n",
    "    \n",
    "  def row_count(self):\n",
    "    return self.dataframe.count()\n",
    "  \n",
    "  def regex_tokenize(self):\n",
    "    from pyspark.ml.feature import RegexTokenizer\n",
    "    from pyspark.ml.feature import Tokenizer\n",
    "    from pyspark.sql.functions import col, udf\n",
    "    from pyspark.sql.types import IntegerType\n",
    "    tweet_regexTokenizer = RegexTokenizer(inputCol=\"_c2\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "    #tweet_countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "    self.regexTokenized = tweet_regexTokenizer.transform(self.dataframe)\n",
    "    return self.regexTokenized\n",
    "    \n",
    "  def regex_tokenize_show(self, truncation=True):  \n",
    "    self.regexTokenized.select(\"words\").show(truncate=truncation)\n",
    "  \n",
    "  def stop_words_remove(self, stop=[]):\n",
    "    from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "    tweet_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_raw\")\n",
    "    tweet_removed = tweet_remover.transform(self.regexTokenized)\n",
    "    self.stop_removed = tweet_removed\n",
    "    \n",
    "    tweet_remover_adv = StopWordsRemover(inputCol=\"filtered_raw\", outputCol=\"filtered\", stopWords=stop)\n",
    "    self.tweet_removed_adv = tweet_remover_adv.transform(tweet_removed)\n",
    "    self.tweet_removed_adv = self.tweet_removed_adv.dropna()\n",
    "    return self.tweet_removed_adv\n",
    "  \n",
    "  def word2vector(self, feature_num=10000):\n",
    "    from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "    tweet_hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=feature_num)\n",
    "    tweet_featurizedData = tweet_hashingTF.transform(self.tweet_removed_adv)\n",
    "\n",
    "    tweet_idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    tweet_idfModel = tweet_idf.fit(tweet_featurizedData)\n",
    "    tweet_rescaledData = tweet_idfModel.transform(tweet_featurizedData)\n",
    "\n",
    "    tweet_svms = tweet_rescaledData.selectExpr(\"features\")\n",
    "    self.tweet_svms=tweet_svms.dropna()\n",
    "    \n",
    "  def nb_predicting(self):\n",
    "    from pyspark.ml.classification import NaiveBayes,NaiveBayesModel\n",
    "    from pyspark.mllib.util import MLUtils\n",
    "\n",
    "    self.tweet_result = self.model.transform(self.tweet_svms)  \n",
    "    #tweet_predictionAndLabels = tweet_result.select(\"prediction\")   \n",
    "  \n",
    "  def nb_predicting_show(self):\n",
    "    self.tweet_result.select(\"prediction\",\"features\").show(truncate=True)\n",
    "    \n",
    "  def result(self):\n",
    "    sqlContext.registerDataFrameAsTable(self.tweet_result, \"table1\")\n",
    "    self.positive = sqlContext.sql(\"SELECT count(*) from table1 where prediction=1 group by prediction\")\n",
    "    self.negative = sqlContext.sql(\"SELECT count(*) from table1 where prediction=0 group by prediction\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brand - Coca-cola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data - Coca-Cola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_list=[\"dbfs:/FileStore/tables/53m9hj5w1507394714906/1_1-d7876.txt\", \"dbfs:/FileStore/tables/53m9hj5w1507394714906/1_2-b8a26.txt\", \"dbfs:/FileStore/tables/53m9hj5w1507394714906/2_1-6a057.txt\", \"dbfs:/FileStore/tables/53m9hj5w1507394714906/2_2-08a2a.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/3_1-3ead6.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/3_2-80939.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/4_1-fa2a9.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/4_2-a3648.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/5_1-377eb.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/5_2-dce00.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/6_1-486e2.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/6_2-45e97.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/7_1-b307e.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/7_2-74996.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/8_1-b4602.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/8_2-dc3bd.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/9_1-9fbd5.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/9_2-53f30.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/10_1-d72c3.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/10_2-953fe.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/11_1-10e51.txt\",\"dbfs:/FileStore/tables/53m9hj5w1507394714906/11_2-f6fb2.txt\"]\n",
    "path_account = len(path_list)\n",
    "print(\"file account=\", path_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWordslist=['http','Coca-Cola','Cola','Coca',  'Coke','cola','CocaCola','https','coca','Soft','Plush','Stuffed','Animal','Teddy','http://www','via','YouTube','ebay.to/2isSz6e','https://www','COLA','COCA','video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "for i in range(path_account):\n",
    "  result_dict[i]={'positive':0, 'negative':0}\n",
    "print result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = tweet_sentiment(path_list[0])\n",
    "test_df=test.data_loading()\n",
    "test_df.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing - Coca-Cola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now_file = 0\n",
    "while now_file<path_account:\n",
    "  test = None\n",
    "  test = tweet_sentiment(path_list[now_file])\n",
    "  test_df=test.data_loading()\n",
    "  test_df.take(2)\n",
    "  #print(test.row_count())\n",
    "  test_regex_tokenized = test.regex_tokenize()\n",
    "  #test.regex_tokenize_show(truncation=False)\n",
    "  test_stopremoved=test.stop_words_remove(stop=stopWordslist)\n",
    "  #test_stopremoved.select(\"filtered\").show(truncate=False)\n",
    "  test_svms=test.word2vector(feature_num=10000)\n",
    "  #test.tweet_svms.show()\n",
    "  test.nb_model(model)\n",
    "  test.nb_predicting()\n",
    "  test.result()\n",
    "  posres=test.positive.head(1)\n",
    "  postest_result = int(posres[0][0])\n",
    "  negres=test.negative.head(1)\n",
    "  negtest_result = int(negres[0][0])\n",
    "  result_dict[now_file]={'positive':postest_result, 'negative':negtest_result}\n",
    "  now_file += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_no = 0\n",
    "pos_res_in_month = []\n",
    "neg_res_in_month = []\n",
    "while sample_no<path_account:\n",
    "  if sample_no%2==0:\n",
    "    pos_res_in_month.append(result_dict[sample_no]['positive'])\n",
    "    neg_res_in_month.append(result_dict[sample_no]['negative'])\n",
    "  else:\n",
    "    pos_res_in_month[-1]+=result_dict[sample_no]['positive']\n",
    "    neg_res_in_month[-1]+=result_dict[sample_no]['negative']\n",
    "  sample_no+=1\n",
    "print(\"positive:\", pos_res_in_month)\n",
    "print(\"negative\", neg_res_in_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratio_res = []\n",
    "for mon_cur in range(11):\n",
    "  ratio_res.append(\"%3f\"%(float(pos_res_in_month[mon_cur])/float(neg_res_in_month[mon_cur])))\n",
    "print(ratio_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brand - Pepsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_list2=[\"dbfs:/FileStore/tables/0h7dq7vp1507485401798/1_1-d7876.txt\",\"dbfs:/FileStore/tables/0h7dq7vp1507485401798/1_2-b8a26.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/2_1-6a057.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/2_2-08a2a.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/3_1-3ead6.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/3_2-80939.txt\",\n",
    "          \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/4_1-fa2a9.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/4_2-a3648.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/5_1-377eb.txt\",\n",
    "          \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/5_2-dce00.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/6_1-486e2.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/6_2-45e97.txt\",\n",
    "          \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/7_1-b307e.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/7_2-74996.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/8_1-b4602.txt\",\n",
    "          \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/8_2-dc3bd.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/9_1-9fbd5.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/9_2-53f30.txt\",\n",
    "          \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/10_1-d72c3.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/10_2-953fe.txt\", \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/11_1-10e51.txt\",\n",
    "          \"dbfs:/FileStore/tables/0h7dq7vp1507485401798/11_2-f6fb2.txt\"]\n",
    "path_account2 = len(path_list2)\n",
    "print(\"file account=\", path_account2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWordslist2=['Pepsi','pepsi','http','https','like','Coke','can','drink','coke','PEPSI','get','https://www','YouTube','via','need','cola','video','time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_dict2 = {}\n",
    "for i in range(path_account2):\n",
    "  result_dict2[i]={'positive':0, 'negative':0}\n",
    "print result_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = tweet_sentiment(path_list2[0])\n",
    "test_df=test.data_loading()\n",
    "test_df.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now_file = 0\n",
    "while now_file<path_account2:\n",
    "  test = None\n",
    "  test = tweet_sentiment(path_list2[now_file])\n",
    "  test_df=test.data_loading()\n",
    "  #test_df.show()\n",
    "  #print(test.row_count())\n",
    "  test_regex_tokenized = test.regex_tokenize()\n",
    "  #test.regex_tokenize_show(truncation=False)\n",
    "  test_stopremoved=test.stop_words_remove(stop=stopWordslist2)\n",
    "  #test_stopremoved.select(\"filtered\").show(truncate=False)\n",
    "  test_svms=test.word2vector(feature_num=10000)\n",
    "  #test.tweet_svms.show()\n",
    "  test.nb_model(model)\n",
    "  test.nb_predicting()\n",
    "  test.result()\n",
    "  posres=test.positive.head(1)\n",
    "  postest_result = int(posres[0][0])\n",
    "  negres=test.negative.head(1)\n",
    "  negtest_result = int(negres[0][0])\n",
    "  result_dict2[now_file]={'positive':postest_result, 'negative':negtest_result}\n",
    "  now_file += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(result_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_no = 0\n",
    "pos_res_in_month2 = []\n",
    "neg_res_in_month2 = []\n",
    "while sample_no<path_account:\n",
    "  if sample_no%2==0:\n",
    "    pos_res_in_month2.append(result_dict2[sample_no]['positive'])\n",
    "    neg_res_in_month2.append(result_dict2[sample_no]['negative'])\n",
    "  else:\n",
    "    pos_res_in_month2[-1]+=result_dict2[sample_no]['positive']\n",
    "    neg_res_in_month2[-1]+=result_dict2[sample_no]['negative']\n",
    "  sample_no+=1\n",
    "print(\"positive:\", pos_res_in_month2)\n",
    "print(\"negative\", neg_res_in_month2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratio_res2 = []\n",
    "for mon_cur2 in range(11):\n",
    "  ratio_res2.append(\"%3f\"%(float(pos_res_in_month2[mon_cur2])/float(neg_res_in_month2[mon_cur2])))\n",
    "print(ratio_res2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "proj_fin_simplified",
  "notebookId": 1033259491569576
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
